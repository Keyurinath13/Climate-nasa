{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keyurinath13/Climate-nasa/blob/main/climate_nasa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the LightGBM library\n",
        "!pip install lightgbm\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style('whitegrid')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.1)\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "TyaXpQqWMkPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31514e2-0bdb-4d0a-a5eb-f272be620b22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 2\\. Data Loading\n",
        "\n",
        "As before, load the training and test datasets. Ensure the CSV files are in the same directory as your Jupyter Notebook."
      ],
      "metadata": {
        "id": "t-ZcK5ViMkPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets from your local directory\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Execution halted: Make sure 'train.csv' and 'test.csv' are in the correct folder.\")\n",
        "    train_df = pd.DataFrame() # Stop further execution if files are not found\n",
        "\n",
        "# Display shapes to confirm they loaded correctly\n",
        "if not train_df.empty:\n",
        "    print(f\"Train shape: {train_df.shape}\")\n",
        "    print(f\"Test shape: {test_df.shape}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "snx3S1wHMkPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 3\\. Advanced Preprocessing & Feature Engineering\n",
        "\n",
        "This is where we'll implement a much more robust data preparation pipeline.\n",
        "\n",
        "### Step 3.1: Handle Skewed Target Variable\n",
        "\n",
        "[cite\\_start]Your EDA showed the `emission` data is highly skewed[cite: 345]. A log transform will make its distribution more normal, which helps the model learn more effectively. We use `np.log1p` which is equivalent to `log(1 + x)` to handle potential zero values."
      ],
      "metadata": {
        "id": "By6dPpQAMkPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'emission' in train_df.columns:\n",
        "    # Visualize original vs. transformed distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    sns.histplot(train_df['emission'], kde=True, ax=axes[0], bins=50)\n",
        "    axes[0].set_title('Original Emission Distribution')\n",
        "\n",
        "    # Apply log transformation\n",
        "    train_df['emission_log'] = np.log1p(train_df['emission'])\n",
        "\n",
        "    sns.histplot(train_df['emission_log'], kde=True, ax=axes[1], bins=50)\n",
        "    axes[1].set_title('Log-Transformed Emission Distribution')\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qL4ooJ0NMkPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformed distribution is much closer to a bell curve, which is ideal for many regression models.\n",
        "\n",
        "### Step 3.2: Feature Selection and Cleaning\n",
        "\n",
        "[cite\\_start]Your EDA revealed that many columns are almost entirely empty (99% missing)[cite: 448, 452]. It's better to remove these than to try and fill them. For the rest, we will fill missing values using the **median**."
      ],
      "metadata": {
        "id": "Dc4eTb6PMkPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and drop the ID column and the original emission target\n",
        "train_df = train_df.drop(columns=['ID', 'emission'])\n",
        "test_ids = test_df['ID'] # Keep test IDs for submission\n",
        "test_df = test_df.drop(columns=['ID'])\n",
        "\n",
        "# Identify columns with a high percentage of missing values from the training data\n",
        "missing_pct = train_df.isnull().sum() / len(train_df)\n",
        "cols_to_drop = missing_pct[missing_pct > 0.90].index\n",
        "\n",
        "train_df = train_df.drop(columns=cols_to_drop)\n",
        "test_df = test_df.drop(columns=cols_to_drop)\n",
        "\n",
        "print(f\"Dropped {len(cols_to_drop)} columns with >90% missing values.\")\n",
        "\n",
        "# Select only numeric features for imputation and modeling\n",
        "numeric_features = train_df.select_dtypes(include=np.number).columns.tolist()\n",
        "numeric_features.remove('emission_log') # Remove the target\n",
        "\n",
        "# Use SimpleImputer to fill remaining NaNs with the median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Fit on training data and transform both train and test data\n",
        "train_df[numeric_features] = imputer.fit_transform(train_df[numeric_features])\n",
        "test_df[numeric_features] = imputer.transform(test_df[numeric_features])\n",
        "\n",
        "print(\"Filled remaining missing values using the median.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vgW4d9XoMkPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 4\\. Advanced Modeling with LightGBM and Cross-Validation\n",
        "\n",
        "We'll now train the LightGBM model. Using **K-Fold Cross-Validation** gives a more accurate measure of performance by training and testing the model on different subsets (\"folds\") of the data."
      ],
      "metadata": {
        "id": "MrTL563iMkPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X = train_df[numeric_features]\n",
        "y = train_df['emission_log']\n",
        "X_test = test_df[numeric_features]\n",
        "\n",
        "# Model parameters for LightGBM\n",
        "# These are a good starting point; they can be tuned for even better performance\n",
        "params = {\n",
        "    'objective': 'regression_l1', # L1 loss is robust to outliers\n",
        "    'metric': 'rmse',\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 0.1,\n",
        "    'num_leaves': 31,\n",
        "    'verbose': -1,\n",
        "    'n_jobs': -1,\n",
        "    'seed': 42,\n",
        "    'boosting_type': 'gbdt',\n",
        "}\n",
        "\n",
        "# Set up K-Fold cross-validation\n",
        "NFOLDS = 5\n",
        "folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(X.shape[0]) # To store out-of-fold predictions\n",
        "sub_preds = np.zeros(X_test.shape[0]) # To store test predictions\n",
        "\n",
        "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_valid, y_valid = X.iloc[valid_idx], y.iloc[valid_idx]\n",
        "\n",
        "    # Define the model\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_valid, y_valid)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "    # Store predictions\n",
        "    oof_preds[valid_idx] = model.predict(X_valid)\n",
        "    sub_preds += model.predict(X_test) / folds.n_splits\n",
        "\n",
        "    print(f\"Fold {n_fold+1} validation RMSE: {np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx])):.4f}\")\n",
        "\n",
        "# Overall validation score\n",
        "overall_rmse = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"\\nOverall Cross-Validation RMSE: {overall_rmse:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "v59vXR7rMkPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 5\\. Advanced Evaluation and Feature Importance\n",
        "\n",
        "Now let's evaluate our results and see which features the model found most important.\n",
        "\n",
        "### Step 5.1: Performance Metrics\n",
        "\n",
        "Since we transformed our target variable, we must transform the predictions back to the original scale using `np.expm1` before calculating the final error metrics. This makes the results interpretable."
      ],
      "metadata": {
        "id": "zH-HZmndMkPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform predictions and actuals back to the original scale\n",
        "y_original = np.expm1(y)\n",
        "oof_preds_original = np.expm1(oof_preds)\n",
        "\n",
        "# Calculate final metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_original, oof_preds_original))\n",
        "mae = mean_absolute_error(y_original, oof_preds_original)\n",
        "r2 = r2_score(y_original, oof_preds_original)\n",
        "\n",
        "print(f\"Overall Metrics on Original Scale:\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"  R-squared (R2): {r2:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sK8k4YUIMkPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.2: Actual vs. Predicted Plot\n",
        "\n",
        "This plot helps visualize the model's accuracy. A perfect model would have all points lying on the red diagonal line."
      ],
      "metadata": {
        "id": "sIDM8WbVMkPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_original, oof_preds_original, alpha=0.3)\n",
        "plt.plot([y_original.min(), y_original.max()], [y_original.min(), y_original.max()], '--r', linewidth=2)\n",
        "plt.xlabel('Actual Emission')\n",
        "plt.ylabel('Predicted Emission')\n",
        "plt.title('Actual vs. Predicted Emissions (Out-of-Fold)')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zvk96WF6MkPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.3: Feature Importance\n",
        "\n",
        "Let's see which features the model relied on most. [cite\\_start]This is a key part of \"understanding\" climate change factors as per the project goal[cite: 33]."
      ],
      "metadata": {
        "id": "Q4LBLoKCMkPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-train a single model on all data to get feature importances\n",
        "final_model = lgb.LGBMRegressor(**params)\n",
        "final_model.fit(X, y)\n",
        "\n",
        "# Get and plot feature importances\n",
        "importances = pd.DataFrame({'feature': X.columns, 'importance': final_model.feature_importances_})\n",
        "importances = importances.sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='importance', y='feature', data=importances)\n",
        "plt.title('Top 20 Feature Importances')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zDwExgo7MkPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart clearly shows which sensor readings and location/time data were most predictive, providing valuable insights."
      ],
      "metadata": {
        "id": "dq5uhaiWMkPS"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}